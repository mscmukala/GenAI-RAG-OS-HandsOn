{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691082db",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "#install oracle ads \n",
    "!pip install oracle-ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3dc833",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "import ads\n",
    "from ads.dataset.factory import DatasetFactory\n",
    "from ads.evaluations.evaluator import ADSEvaluator  \n",
    "from ads.common.data import ADSData\n",
    "from ads.common.model_artifact import ModelArtifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fd3ac",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "!pip install oracle-automlx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707dcc0d",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "import automlx as automl\n",
    "from automlx import Pipeline\n",
    "automl.init(engine='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82590f44",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ[\"NB_SESSION_COMPARTMENT_OCID\"])\n",
    "print(os.environ[\"PROJECT_OCID\"])\n",
    "print(os.environ[\"USER_OCID\"])\n",
    "print(os.environ[\"TENANCY_OCID\"])\n",
    "print(os.environ[\"NB_REGION\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182fbbe1",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "# Put your compartment id\n",
    "compartment_id = s.environ[\"NB_SESSION_COMPARTMENT_OCID\"]\n",
    "# OCI \n",
    "genai_endpoint = \"https://inference.generativeai.us-ashburn-1.oci.oraclecloud.com\"\n",
    "# model_id for embedding \n",
    "genai_embedding_model =\"cohere.embed-english-v3.0\"\n",
    "# model_id for generation\n",
    "oci_model = \"cohere.command-r-plus\" \n",
    "\n",
    "# pre provsioned opensearch_url ( do not change)\n",
    "opensearch_url=\"https://amaaaaaaedf3kyya4llgnrumbw6newbj5ihvy4cg64swrbiq3rgpufkuao4q.opensearch.us-ashburn-1.oci.oraclecloud.com:9200\"\n",
    "# Setup OpenSearch Username & Password: these are only valid during the live-lab.  \n",
    "username=\"adminos\"\n",
    "password=\"Asp12345$\"\n",
    "#will change later with diffrent dataset\n",
    "index_name = \"train-inx\"\n",
    "auth = (username, password)\n",
    "BULK_LIMIT=10\n",
    "AUTH_TYPE=\"RESOURCE_PRINCIPAL\"\n",
    "#will change later with diffrent dataset\n",
    "file_path = 'data/medical-reasoning'\n",
    "\n",
    "# Setup Resource Principal for authentication\n",
    "auth_provider = oci.auth.signers.get_resource_principals_signer()\n",
    "MAX_DOCUMENTS = 1000\n",
    "# Set up Oracle ADS for authentication\n",
    "ads.set_auth(\"resource_principal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd01f1",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "#using huggingface embedding model (facing issues accessing OCI emedding model)\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e6f961",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "# Initialize OpenSearch as the vector database\n",
    "vector_db = OpenSearchVectorSearch(opensearch_url=opensearch_url, \n",
    "                            index_name=index_name, \n",
    "                            embedding_function=embeddings,\n",
    "                            signer=auth_provider,\n",
    "                            auth_type=AUTH_TYPE,\n",
    "                            http_auth=auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f2d9bd",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "# Updated function to process medical reasoning dataset\n",
    "def process_medical_reasoning_data(file_path):\n",
    "    \"\"\"\n",
    "    Process medical reasoning data from JSONL files in the specified directory\n",
    "    Each line contains: Question, Complex_CoT, and Response fields\n",
    "    \"\"\"\n",
    "    import os\n",
    "    import json\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    documents = []\n",
    "    cnt = 0\n",
    "    \n",
    "    # Check if file_path is a directory or file\n",
    "    if os.path.isdir(file_path):\n",
    "        # Process all JSONL files in the directory\n",
    "        for filename in os.listdir(file_path):\n",
    "            if filename.endswith('.jsonl'):\n",
    "                file_full_path = os.path.join(file_path, filename)\n",
    "                print(f\"Processing file: {filename}\")\n",
    "                cnt = process_jsonl_file(file_full_path, documents, cnt)\n",
    "                if MAX_DOCUMENTS > 0 and cnt >= MAX_DOCUMENTS:\n",
    "                    break\n",
    "    else:\n",
    "        # Process single file\n",
    "        if file_path.endswith('.jsonl'):\n",
    "            print(f\"Processing single file: {file_path}\")\n",
    "            cnt = process_jsonl_file(file_path, documents, cnt)\n",
    "    \n",
    "    print(f\"Total documents processed: {cnt}\")\n",
    "    return documents\n",
    "\n",
    "def process_jsonl_file(file_path, documents, cnt):\n",
    "    \"\"\"\n",
    "    Process a single JSONL file and extract medical reasoning data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            for line_num, line in enumerate(file, 1):\n",
    "                try:\n",
    "                    # Parse each line as JSON\n",
    "                    data = json.loads(line.strip())\n",
    "                    \n",
    "                    # Extract question, reasoning, and response\n",
    "                    question = data.get('Question', '')\n",
    "                    complex_cot = data.get('Complex_CoT', '')\n",
    "                    response = data.get('Response', '')\n",
    "                    \n",
    "                    # Create comprehensive document content\n",
    "                    content_parts = []\n",
    "                    if question:\n",
    "                        content_parts.append(f\"Question: {question}\")\n",
    "                    if complex_cot:\n",
    "                        content_parts.append(f\"Reasoning: {complex_cot}\")\n",
    "                    if response:\n",
    "                        content_parts.append(f\"Answer: {response}\")\n",
    "                    \n",
    "                    # Combine all parts\n",
    "                    full_content = \"\\n\\n\".join(content_parts)\n",
    "                    \n",
    "                    if full_content.strip():  # Only add non-empty documents\n",
    "                        documents.append(Document(\n",
    "                            page_content=full_content,\n",
    "                            metadata={\n",
    "                                \"source\": file_path,\n",
    "                                \"line_number\": line_num,\n",
    "                                \"question\": question[:100] + \"...\" if len(question) > 100 else question\n",
    "                            }\n",
    "                        ))\n",
    "                        cnt += 1\n",
    "                        \n",
    "                        if MAX_DOCUMENTS > 0 and cnt >= MAX_DOCUMENTS:\n",
    "                            return cnt\n",
    "                            \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Error parsing line {line_num} in {file_path}: {e}\")\n",
    "                    continue\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "    \n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250ff54",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "MAX_DOCUMENTS = 100 # Override the limit to ingest only first 100 just for demo. Comment this line out or set to negative number to ingest all documents.\n",
    "\n",
    "# Process medical reasoning documents \n",
    "documents = process_medical_reasoning_data(file_path)\n",
    "print(f\"\\nValidate Processed documents by printing a few random documents:\")\n",
    "if len(documents) > 0:\n",
    "    print(f\"\\nDOCUMENT 1:\\n{documents[0].page_content[:500]}...\")\n",
    "    if len(documents) > 50:\n",
    "        print(f\"\\nDOCUMENT 51:\\n{documents[50].page_content[:500]}...\")\n",
    "    if len(documents) > 99:\n",
    "        print(f\"\\nDOCUMENT 100:\\n{documents[99].page_content[:500]}...\")\n",
    "\n",
    "print(f\"\\nTotal Number of Documents to ingest: {len(documents)}\")\n",
    "print(f\"Document structure - each contains Question, Reasoning, and Answer sections from medical cases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a765a9b",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "# Check the index mapping\n",
    "response = vector_db.client.indices.get_mapping(index=index_name)\n",
    "print(\"Index Mapping:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88d1bd",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to perform a semantic search using vector embeddings\n",
    "def retrieve_documents_with_embeddings(query, top_k=5):\n",
    "    # Generate the embedding for the query using your embedding function\n",
    "    query_embedding = vector_db.embedding_function.embed_query(query)\n",
    "    \n",
    "    # Ensure the embedding is in the correct format (e.g., a list of floats)\n",
    "    query_embedding = np.array(query_embedding).tolist()\n",
    "\n",
    "    # Perform a knn search in OpenSearch\n",
    "    search_results = vector_db.client.search(\n",
    "        index=vector_db.index_name,\n",
    "        body={\n",
    "            \"size\": top_k,\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"vector_field\": {  # Use the correct field name for embeddings\n",
    "                        \"vector\": query_embedding,\n",
    "                        \"k\": top_k\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    documents_with_embeddings = []\n",
    "    for hit in search_results['hits']['hits']:\n",
    "        doc_content = hit['_source']['text']  # Adjust to the correct field name for document text\n",
    "        embedding = hit['_source'].get('vector_field')  # Retrieve the embedding if needed\n",
    "        documents_with_embeddings.append((doc_content, embedding))\n",
    "\n",
    "    return documents_with_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf255b7",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "# Example usage\n",
    "query = \"Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\"\n",
    "documents_with_embeddings = retrieve_documents_with_embeddings(query,2)\n",
    "\n",
    "# Print the documents and their embeddings\n",
    "print(f\"Top {len(documents_with_embeddings)} documents and their embeddings for the query: \\\"{query}\\\"\")\n",
    "for idx, (content, embedding) in enumerate(documents_with_embeddings):\n",
    "    print(f\"\\nDocument {idx + 1}:\")\n",
    "    print(f\"Content: {content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8226e2",
   "metadata": {
    "vscode": {
     "languageId": "json"
    }
   },
   "outputs": [],
   "source": [
    "# Semantic Search Test Function\n",
    "def semantic_search_test(query, top_k=5):\n",
    "    # Perform a semantic search\n",
    "    search_results = vector_db.similarity_search(query, k=top_k)\n",
    "    \n",
    "    # Display the top-k retrieved documents\n",
    "    print(f\"Top {top_k} results for the query: \\\"{query}\\\"\")\n",
    "    for idx, result in enumerate(search_results):\n",
    "        print(f\"\\nResult {idx + 1}:\")\n",
    "        print(f\"Document: {result.page_content}\\n\")\n",
    "\n",
    "# Run a semantic search test\n",
    "semantic_search_test(\"Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\", top_k=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
